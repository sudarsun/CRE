/*
 * Boost Software License - Version 1.0 - August 17th, 2003
 *
 * Permission is hereby granted, free of charge, to any person or organization
 * obtaining a copy of the software and accompanying documentation covered by
 * this license (the "Software") to use, reproduce, display, distribute,
 * execute, and transmit the Software, and to prepare derivative works of the
 * Software, and to permit third-parties to whom the Software is furnished to
 * do so, all subject to the following:
 *
 * The copyright notices in the Software and this entire statement, including
 * the above license grant, this restriction and the following disclaimer,
 * must be included in all copies of the Software, in whole or in part, and
 * all derivative works of the Software, unless such copies or derivative
 * works are solely in the form of machine-executable object code generated by
 * a source language processor.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
 * SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
 * FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
 * DEALINGS IN THE SOFTWARE.
 *
 */

#include "ClassRatioEstimator.hpp"
#include "Kernel.hpp"
#include "QuadProg.hpp"
#include "Stopwatch.hpp"
#include "Data.hpp"
#include "Utils.hpp"
#include <vector>
#include <cfloat>
#include <boost/filesystem/operations.hpp>
#include <armadillo>
#include "LibSVMFormat.hpp"

static const float kCorrelationThreshold = 0.90;

float ClassRatioEstimator::BandwidthSelect(const Matrix& inMatrix) const
{
	int nsamples = inMatrix.Rows();
	int dimensions = inMatrix.Columns();

	float sigma = 0;
	if ( nsamples < 500 )
		sigma = 0.4 * sqrt(dimensions);
	else if ( nsamples < 1000 )
		sigma = 0.2 * sqrt(dimensions);
	else
		sigma = 0.14 * sqrt(dimensions);

	float mean_std_x = 0;
	if ( dimensions > 1E4 )
	{
		int_array colinds;
		for ( int i = 0; i < dimensions; i+=10 )
			colinds.push_back(i);

		mean_std_x = Mean(StandardDeviation(inMatrix, colinds));
	}
	else
	{
		mean_std_x = Mean(StandardDeviation(inMatrix));
	}

	return mean_std_x * sigma;
}

int PP = -1;
void ClassRatioEstimator::BestKernel(const Data& inTrain, const Data &inEval, real_array& outWeights, ScorerType inType )
{
	const Matrix &train = inTrain.Features();
	const Matrix &eval = inEval.Features();

	if ( train.Columns() != eval.Columns() )
		throw std::runtime_error("feature column count disagreement");

	const Matrix &yte = inEval.Labels();
	real_array yte_props = ClassProportions(yte);

	int noClasses = inTrain.MaxLabel();

	int dims = train.Columns();
	int n1 = train.Rows(), n2 = eval.Rows();

	float bandwidth = BandwidthSelect( train );

	float sim_score = -FLT_MIN;
	int best_kernel = -1;
	real_array best_props;

	DenseMatrix superKrr(n1,n1), superKer(n2,n1);
	float superWt = 0;

	// create a placeholder for the weight proportions.
	outWeights.resize( dims + 13 );

	DenseMatrix Krr, Ker;
	RBFKernel kernel( bandwidth );
	int k = 0;
	for ( ; k < dims; ++k )
	{
		// have only one dimension in the column select.
		int_array cols;
		cols.push_back(k);

		Stopwatch sw;
		// compute train-train and eval-train kernels.
		kernel.Compute(train, train, Krr, cols );
		float krr_time = sw.Elapsed();

		sw.Restart();
		kernel.Compute(eval, train, Ker, cols );
		float ker_time = sw.Elapsed();

		PP = k;
		sw.Restart();
		real_array props_estimated(noClasses);
		if ( !MMD( dynamic_cast<const DenseMatrix &>(inTrain.Labels()), noClasses, Krr, Ker, props_estimated ) )
			continue;

		float mmd_time = sw.Elapsed();

		float sim_score_now = Score(yte_props, props_estimated, inType ); // L1Score( yte_props, props_estimated );
		if ( sim_score_now > sim_score )
		{
			best_kernel = k;
			sim_score = sim_score_now;
			best_props = props_estimated;
		}

		if ( sim_score_now >= kCorrelationThreshold )
		{
			Krr *= sim_score_now;
			superKrr += Krr;

			Ker *= sim_score_now;
			superKer += Ker;

			superWt += sim_score_now;

			outWeights[k] = sim_score_now;
		}

		std::cout << "Dim[" << k << "] SIM=" << sim_score_now << " {bestSIM=" << sim_score << "(" << best_kernel << ")} "
					<< "krr: " << krr_time << "mS "
					<< "ker: " << ker_time << "mS "
					<< "mmd: " << mmd_time << "mS ";

		if ( sim_score_now >= kCorrelationThreshold )
			std::cout << "SELECTED ";

		std::cout << std::endl;
	}

	for ( int i = -6; i <= 6; ++i, ++k )
	{
		float bw = pow(2,i) * bandwidth * dims * dims;
		RBFKernel kernel( bw );

		// compute train-train and eval-train kernels.
		Stopwatch sw;
		kernel.Compute(train, train, Krr );
		float krr_time = sw.Elapsed();

		sw.Restart();
		kernel.Compute(eval, train, Ker );
		float ker_time = sw.Elapsed();

		PP = k;
		sw.Restart();
		real_array props_estimated;
		if ( !MMD( dynamic_cast<const DenseMatrix &>(inTrain.Labels()), noClasses, Krr, Ker, props_estimated ) )
			continue;

		float mmd_time = sw.Elapsed();

		float sim_score_now = Score( yte_props, props_estimated, inType ); // L1Score( yte_props, props_estimated );
		if ( sim_score_now > sim_score )
		{
			best_kernel = k;
			sim_score = sim_score_now;
			best_props = props_estimated;
		}

		if (sim_score_now >= kCorrelationThreshold)
		{
			Krr *= sim_score_now;
			superKrr += Krr;

			Ker *= sim_score_now;
			superKer += Ker;

			superWt = sim_score_now;

			outWeights[k] = sim_score_now;
		}

		std::cout << "Multi[" << k << "] SIM=" << sim_score_now << " {bestSIM=" << sim_score << "(" << best_kernel << ")} "
					<< "krr: " << krr_time << "mS "
					<< "ker: " << ker_time << "mS "
					<< "mmd: " << mmd_time << "mS ";

		if ( sim_score_now >= kCorrelationThreshold )
			std::cout << "SELECTED ";

		std::cout << std::endl;
	}

	std::cout << "Best Theta:\n" << best_props << std::endl;
	std::cout << "L1 norm: " << LpNorm( best_props, yte_props, 1 ) << std::endl;
	std::cout << "L1 simi: " << L1Score( best_props, yte_props ) << std::endl;
	std::cout << "ModL1 simi: " << ModifiedBinaryL1Score( best_props, yte_props ) << std::endl;
	std::cout << "Cosine : " << Cosine( best_props, yte_props ) << std::endl;
	std::cout << "Correlation: " << Correlation( best_props, yte_props ) << "\n" << std::endl;

	superKrr *= (1.0/superWt);
	superKer *= (1.0/superWt);

	float super_sim_score = -1;
	real_array props_estimated;
	if ( MMD( dynamic_cast<const DenseMatrix &>(inTrain.Labels()), noClasses, superKrr, superKer, props_estimated ))
	{
		std::cout << "Super Theta:\n" << props_estimated << std::endl;

		std::cout << "Super L1 norm: " << LpNorm( props_estimated, yte_props, 1 ) << std::endl;
		std::cout << "Super L1 simi: " << L1Score( props_estimated, yte_props ) << std::endl;
		std::cout << "Super ModL1 simi: " << ModifiedBinaryL1Score( props_estimated, yte_props ) << std::endl;
		std::cout << "Super Cosine: " << Cosine( props_estimated, yte_props ) << std::endl;
		std::cout << "Super Correlation: " << Correlation( props_estimated, yte_props ) << "\n" << std::endl;

		super_sim_score = Score( props_estimated, yte_props, inType );  // L1Score( props_estimated, yte_props );

		// super kernel didn't contribute, so use the best kernel with binary weights.
		// if super kernel is 1% less than single kernel estimate, we shall override,
		// otherwise stick to super kernel.
		if ( (super_sim_score+0.01) < sim_score )
		{
			std::cout << "\nUsed Sparse MKL Kernel!\n" << std::endl;
			outWeights.clear();
			outWeights.resize( dims + 13 );
			outWeights[best_kernel] = 1;
		}
		else
		{
			std::cout << "\nUsed Super Kernel!\n" << std::endl;
		}
	}
	else // super kernel failed, so use the best kernel with binary weights.
	{
		std::cout << "\nUsed Sparse MKL Kernel!\n" << std::endl;
		outWeights.clear();
		outWeights.resize( dims + 13 );
		outWeights[best_kernel] = 1;
	}

}

void
ClassRatioEstimator::GetKernels( const Matrix &inA, const Matrix &inB, DenseMatrix &outKernel, float inBandwidth, const real_array &inWts, const std::string &inPrefix )
{
	int dimensions = inA.Columns();

	real_array bandwidths;
	for (int i = -6; i <= 6; ++i )
	{
		float product = pow(2,i);
		product *= inBandwidth * dimensions * dimensions;
		bandwidths.push_back( product );
	}

	float totalWt = 0;
	outKernel.Resize(inA.Rows(), inB.Rows());
	// initialize the kernel matrix, without which
	// boost gives you a random matrix!
	outKernel.Zeroize();

	int univariateGaussians = dimensions;
	RBFKernel kernel( inBandwidth );
	for ( int k = 0; k < dimensions; ++k )
	{
		// skip kernels that are not required.
		if ( inWts[k] == 0 )
			continue;

		// have only one dimension in the column select.
		int_array cols;
		cols.push_back(k);

		std::cout << "Dim[" << k << "]: ";
		Stopwatch sw;

		DenseMatrix K;

		if ( !inPrefix.length() )
			kernel.Compute( inA, inB, K, cols );
		else
		{
			char filename[100];
			sprintf( filename, "%s-%d.mat", inPrefix.c_str(), k );
			if ( !K.Load(filename) )
			{
				Stopwatch tt;
				// kernel computed on a per dimension basis.
				kernel.Compute( inA, inB, K, cols );
				std::cout << "Compute:" << tt.Restart() << " mS;";
				K.Save(filename);
				std::cout << "Save:" << tt.Restart() << " mS; ";
			}
		}

		K *= inWts[k];
		totalWt += inWts[k];
		outKernel += K;

		std::cout << sw.Elapsed() << " mS" << std::endl;
	}

	int nofKernels = univariateGaussians + bandwidths.size();
	for ( int k = univariateGaussians; k < nofKernels; ++k )
	{
		if ( inWts[k] == 0 )
			continue;

		float bw = bandwidths[ k-univariateGaussians ];
		RBFKernel kernel(bw);

		std::cout << "BW[" << k << "]: ";
		Stopwatch sw;

		DenseMatrix K;

		if ( !inPrefix.length() )
			kernel.Compute( inA, inB, K );
		else
		{
			char filename[100];
			sprintf( filename, "%s-%d.mat", inPrefix.c_str(), k );
			if ( !K.Load(filename) )
			{
				Stopwatch tt;
				kernel.Compute( inA, inB, K );
				std::cout << "Compute:" << tt.Restart() << " mS;";
				K.Save(filename);
				std::cout << "Save:" << tt.Restart() << " mS; ";
			}
		}

		K *= inWts[k];
		totalWt += inWts[k];
		outKernel += K;

		std::cout << sw.Elapsed() << " mS" << std::endl;
	}

	outKernel *= (1.0/totalWt);
}
/*
void ClassRatioEstimator::GetKernels(const Matrix& inA, const Matrix& inB, dense_matrix_array &outKernels, float inBandwidth, bool inMulti, KernelImplType inType )
{
	int dimensions = inA.Columns();

	real_array bandwidths;
	for (int i = -6; i <= 6; ++i )
	{
		float product = pow(2,i);
		product *= inBandwidth * dimensions * dimensions;
		bandwidths.push_back( product );
	}

	int univariateGaussians = dimensions;
	if ( inMulti )
		univariateGaussians = 0;
	else
	{
		std::auto_ptr<Kernel> kernel( new RBFKernel( inBandwidth ) );

		outKernels.resize( dimensions );
		for ( int k = 0; k < dimensions; ++k )
		{
			// have only one dimension in the column select.
			int_array cols;
			cols.push_back(k);

			std::cerr << "Dim[" << k << "]: ";
			Stopwatch sw;
			// kernel computed on a per dimension basis.
			kernel->Compute( inA, inB, outKernels[k], cols );
			std::cerr << sw.Elapsed() << " mS" << std::endl;
		}

	}

	int nofKernels = univariateGaussians + bandwidths.size();
	outKernels.resize( nofKernels );
	for ( int k = univariateGaussians; k < nofKernels; ++k )
	{
		float bw = bandwidths[ k-univariateGaussians ];
		std::auto_ptr<Kernel> kernel( new RBFKernel( inBandwidth ) );

		std::cerr << "BW[" << k << "]: ";
		Stopwatch sw;
		kernel->Compute( inA, inB, outKernels[k] );
		std::cerr << sw.Elapsed() << " mS" << std::endl;
	}
}*/

/**
 * function [theta, obj] = MMD(Y, c, Krr, Ker)

	H = zeros(c);
	f = zeros(c, 1);
	for i = 1:c
		idx1 = Y == i;
		for j = 1:c
			idx2 = Y == j;
	%         if i == j
	%             Ktemp = Krr(idx1, idx2);
	%             H(i, j) = 2*(sum(sum((1 - eye(size(Ktemp))) .* Ktemp)) / (size(Ktemp, 1) * (size(Ktemp, 1) - 1)));
	%         else
				H(i, j) = 2*mean(mean(Krr(idx1, idx2)));
	%         end
		end
		f(i) = -2 * mean(mean(Ker(:, idx1)));
	end

	H = 0.5*(H + H');
	A = [];
	b = [];
	Aeq = ones(1, c);
	beq = 1;
	lb = zeros(c, 1);
	ub = ones(c, 1);
	theta0 = ones(1, c) / c;

	options = optimset('Algorithm', 'active-set', 'Display', 'Off', 'LargeScale', 'Off');

	theta = quadprog(H,f,A,b,Aeq,beq,lb,ub,theta0,options);
	theta = theta./sum(theta);

	obj = .5 * (theta'*H*theta) + theta'*f;
*/

bool ClassRatioEstimator::MMD(const DenseMatrix &inY_, int inClasses, const DenseMatrix& inKrr, const DenseMatrix& inKre, real_array &outValues)
{
	int N = inY_.Rows();
	int_array inY(N);
	for ( int i = 0; i < N; ++i )
		inY[i] = inY_(i,0);

	DenseMatrix H( inClasses, inClasses );
	double_array f( inClasses );

	for ( int i = 0; i < inClasses; ++i )
	{
		int_array idx1 =  Indices( inY, i+1 );
		int size1 = idx1.size();

		for ( int j = 0; j < inClasses; ++j )
		{
			int_array idx2 = Indices( inY, j+1 );
			int size2 = idx2.size();
			DenseMatrix ktemp = inKrr.Select(idx1, idx2);

			/*
			if ( i == j )
			{
				// H(i, j) = 2*(sum(sum( (1 - eye(size(Ktemp))) .* Ktemp)) / (size(Ktemp, 1) * (size(Ktemp, 1) - 1)));

				// 1 - eye(size(Ktemp)) := -1 * (eye(size(Ktemp))-1)
				IdentityMatrix eye( size1, size2 );
				eye -= 1;
				eye *= -1;

				eye *= ktemp;
				float sum = eye.Sum(DenseMatrix::eColWise).Sum(DenseMatrix::eRowWise)(0,0);
				float denom = (size1 * (size1 - 1.0));

				H(i, j) = 2.0 * sum / denom;
			}
			else */
			{
				H(i,j) = ktemp.Mean(DenseMatrix::eColWise).Mean(DenseMatrix::eRowWise)(0,0) * 2.0;
				/*if ( H(i,j) > 100 or H(i,j) < -100 )
				{
					std::cout << "PROBLEM KTEMP\n";
					ktemp >> std::cout;
					exit(0);
				}*/
			}
		}

		int_array dummy;
		DenseMatrix ktemp = inKre.Select(dummy, idx1);
		f[i] = -2.0 * ktemp.Mean(DenseMatrix::eColWise).Mean(DenseMatrix::eRowWise)(0,0);
	}

	//DenseMatrix H1 = H;
	H += H.Transpose();
	H *= 0.5;

	Qdata q(H.Data());
	int result = QuadProg( q, &f[0], inClasses, outValues );

	if ( result )
	{
	/*	std::cout << "PERTURBING..\n";
		H1 >> std::cout;
		std::cout << "\n" << f << std::endl;
		std::cout << std::endl;
		H.Perturb(1000.0);
		H >> std::cout;
		std::cout << std::endl;

		Qdata q1(H);
		result = QuadProg( q1, &f[0], inClasses, outValues );

		if ( result )
			std::cout << "FAILED..\n";
		else
			std::cout << "WORKED..\n";*/
	}

/*
 * @NOTE: http://in.mathworks.com/matlabcentral/fileexchange/35938-converts-a-non-positive-definite-symmetric-matrix-to-positive-definite-symmetric-matrix/content/topdm.m

	if ( result )
	{
		using namespace arma;
		std::cerr << "Converting Objective Matrix to PSD!..";

		mat qq(inClasses, inClasses);
		for ( int j = 0; j < inClasses; ++j )
			for ( int i = 0; i < inClasses; ++i )
				qq(i,j) = H(i,j);

		vec eigval;
		mat eigvec;

		eig_sym(eigval, eigvec, qq);

		for ( int i = 0; i < eigval.n_elem; ++i )
		{
			std::cerr << "eigen->" << i << ": " << eigval(i) << std::endl;
			if ( eigval(i) < 10E-10 )
				eigval(i) = 10E-6;
		}

		mat qq_psd = eigvec*diagmat(eigval)*eigvec.t();
		Qdata q1(qq_psd);
		result = QuadProg( q1, &f[0], inClasses, outValues );

		if ( result )
			std::cerr << "FAILED" << std::endl;
		else
			std::cerr << "WORKED!!" << std::endl;
	}
*/
	return result == 0;
}

